import numpy as np
import matplotlib.pyplot as plt

# Configuración del laberinto
maze_size = 100
maze = np.zeros((maze_size, maze_size))
maze[1:maze_size-1, 1] = 1  # Obstáculos
maze[-1, -1] = 2  # Meta

# Definición de acciones
actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Arriba, abajo, izquierda, derecha

# Función para validar movimientos
def is_valid(state):
    row, col = state
    return 0 <= row < maze_size and 0 <= col < maze_size and maze[row, col] != 1

# Q-Learning
def q_learning(alpha, gamma, epsilon, episodes):
    Q = np.zeros((maze_size, maze_size, len(actions)))
    for episode in range(episodes):
        state = (0, 0)  # Inicio en la esquina superior izquierda
        while maze[state[0], state[1]] != 2:  # Mientras no se llegue a la meta
            # Selección de acción
            if np.random.rand() < epsilon:
                action = np.random.choice(len(actions))  # Exploración
            else:
                action = np.argmax(Q[state[0], state[1]])  # Explotación

            # Nuevo estado
            next_state = (state[0] + actions[action][0], state[1] + actions[action][1])

            # Validación del nuevo estado
            if is_valid(next_state):
                if maze[next_state[0], next_state[1]] == 2:
                    reward = 100  # Meta
                else:
                    reward = -0.1  # Movimiento válido
                # Actualización de Q
                Q[state[0], state[1], action] += alpha * (
                    reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action]
                )
                state = next_state
            else:
                reward = -10  # Penalización por movimiento inválido
        # Salir si la meta fue alcanzada en un episodio
        if maze[state[0], state[1]] == 2:
            break
    return Q

# Encuentra el camino óptimo
def find_path(Q):
    state = (0, 0)
    path = [state]
    while maze[state[0], state[1]] != 2:
        action = np.argmax(Q[state[0], state[1]])
        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])
        if not is_valid(next_state):
            break
        path.append(next_state)
        state = next_state
    return path

# Parámetros
alpha = 0.1
gamma = 0.9
epsilon = 0.5  # Aumento temporal de la exploración
episodes = 100  # Reducción de episodios para prueba

# Entrenamiento
Q = q_learning(alpha, gamma, epsilon, episodes)

# Encontrar el camino óptimo
path = find_path(Q)

# Visualización
plt.imshow(maze, cmap="binary")  
plt.plot([p[1] for p in path], [p[0] for p in path], 'r')  # Camino en rojo
plt.title("Camino encontrado")
plt.show()
